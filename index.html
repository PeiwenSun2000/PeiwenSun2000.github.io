<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Peiwen Sun</title>
  
  <meta name="author" content="Peiwen Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">

<style>
  #clustrmaps {
    display: block;
    margin: 0 auto;
    max-width: 40%;
    height: auto;
  }
</style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Peiwen Sun</name>
		<br>
              </p>
              <p>I am a Researcher at BUPT, focused on machine learning for emotion/sentiment understanding and multimodal understanding.  After I graduated from BUPT, I was recommended to the College of artificial intelligence in BUPT for a master's degree. I have recently been a visiting student at Renmin University of China supervised by Di Hu. Heading to my PhD in future.
		    </p>
		    <p>
              I used to be an intern at Megvii and Tencent to do researches and projects of audio-visual emotion/sentiment understanding and multimodal understanding.
              </p>
              <p>
              
               
              </p>
              <p style="text-align:center">
                <a href="...">CV</a> &nbsp/&nbsp
    <a href="https://github.com/PeiwenSun2000"> GitHub </a> &nbsp/&nbsp 
    <a href="https://peiwensun2000.github.io/"> Page
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/PeiwenSun_headshot.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PeiwenSun_headshot.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<!---
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                2021
                  <ul>

                      <li>Intern at Tencent</li>
                      <li>Facial recognition and emtion/sentiment recognition</li>

                  </ul>
              </p>
              <p>
                2020
                  <ul>

                      <li>Intern at Magvii</li>
                      <li>Facial recognition and emtion/sentiment recognition</li>

                  </ul>
              </p>
            </td>
          </tr>
-->

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <heading>Recent News</heading>

          <p>
            ðŸŽ‰7/16/2024: 2 papers are accepted by ACM MM 2024. See you in Melbourne.
          </p>
          <p>
            ðŸŽ‰7/1/2024: 3 papers are accepted by ECCV 2024. See you in Milan.
          </p>
          </td>
          </tr>


          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Selected Papers</heading>
        
            <p>
      
              My research focuses on audio-visual learning, multi-modal machine learning, with downstream task of person recognition, emtion/sentiment recognition techniques for video recognition, including the use of sound and images to learn better representations.
            </p>
          </td>
            </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<!-- change file in here -->

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/2024AVSbias.png" alt="mvgpt" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2407.16638">
                  <papertitle>Unveiling and Mitigating Bias in Audio Visual Segmentation  </papertitle>
                </a>
                <br>
            <b>PeiwenSun</b>, Honggang Zhang, Di Hu
            <br>
                <br>
                  <b>ACM MM</b> <span style="color:red;">(ORAL)</span>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2407.16638">Arxiv</a>
                <p></p>
                <p>Unveiling and mitigating bias caused by real-world inherent preferences and distributions in Audio Visual Segmention. </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/2024ref-avs.png" alt="mvgpt" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2407.10957">
                  <papertitle>Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes </papertitle>
                </a>
                <br>
                  Yaoting Wang*, <b>PeiwenSun*</b>, Dongzhan Zhou*, Guangyao Li, Honggang Zhang, Di Hu
                <br>
                  *: equal contribution
                <br>
                <br>
                  <b>ECCV</b>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2407.10957">Arxiv</a>
                <p></p>
                <p>A novel task of referring segmenation by audio, visual and temporal information.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/2024teso.png" alt="mvgpt" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2407.10947">
                  <papertitle>Can Textual Semantics Mitigate Sounding Object Segmentation Preference?</papertitle>
                </a>
                <br>
                  Yaoting Wang*, <b>PeiwenSun*</b>, Yuanchao Li, Honggang Zhang, Di Hu
                <br>
                  *: equal contribution
                <br>
                <br>
                  <b>ECCV</b>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2407.10947">Arxiv</a>
                <p></p>
                <p>The ability of LLM can be used to solve segmenation preference on Audio Visual Segmention.</p>
              </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2022fusion.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.isca-archive.org/interspeech_2023/sun23_interspeech.pdf">
                    <papertitle>A Method of Audio-Visual Person Verification by Mining Connections between Time Series  </papertitle>
                  </a>
                  <br>
            <b>PeiwenSun</b>, Shanshan Zhang, Zishan Liu, Yougen Yuan, Taotao Zhang, Honggang Zhang, Pengfei Hu
            <br>
                  <br>
                    <b>INTERSPEECH</b>, 2023
                  <br>
                  <a href="https://www.isca-archive.org/interspeech_2023/sun23_interspeech.pdf">ISCA</a>
                  <p></p>
                  <p>A novel audio-visual strategy in person verification that considers connections between time series from a  generative perspective.</p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2023more.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2312.07212.pdf">
                    <papertitle>More than Vanilla Fusion: a Simple, Decoupling-free, Attention Module for Multimodal fusion Based on Information Theory  </papertitle>
                  </a>
                  <br>
            <b>PeiwenSun</b>, Yifan Zhang, Zishan Liu, Donghao Chen, Honggang Zhang
            <br>
                  <br>
                    <em>arxiv</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2312.07212.pdf">Arxiv</a>
                  <p></p>
                  <p>This paper reconsiders the information fused in the multimodal case from a bionics perspective and proposes a simple, plug-and-play, attention module for vanilla fusion based on fundamental information theory and uncertainty theory.</p>
                </td>
              </tr>

        <!-- </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Ongoing Paper Submissions</heading>
            <p>
              Total: 3 papers
            </p>
            <p>
              ECCV(Ongoing): Mutual contributor on Audio-Visual Segmentation.
            </p>
            <p>
              ECCV(Ongoing): Mutual contributor on A noval task of Multimodal Segmentation.
            </p>
            <p>
              ECCV(Ongoing): Second  contributor on Audio-Visual Segmentation.
            </p>
          </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Other papers</heading>
        
            <p>
              I have been exposed to a variety of research directions such as few-shot learning, medical image processing, mathematical modelling etc.
            </p>
          </td>
            </tr>
        </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2024flashspeech.png" alt="mvgpt" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://flashspeech.github.io/">
                <papertitle>	FlashSpeech: Efficient Zero-Shot Speech Synthesis  </papertitle>
              </a>
              <br>
                Zhen Ye, Zeqian Ju, Haohe Liu, Xu Tan, Jianyi Chen, Yiwen Lu, <b>PeiwenSun</b>, Jiahao Pan, Bianweizhen, Shulin He, Wei Xue, Qifeng Liu, Yike Guo
              <br>
              <br>
                <b>ACM MM</b>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2404.14700">Arxiv</a>
              <p></p>
              <p>A large-scale zero-shot speech synthesis system with approximately 5% of the inference time compared with previous work. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2024stepping.png" alt="mvgpt" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://placeholder.pdf">
                <papertitle>Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation</papertitle>
              </a>
              <br>
                Juncheng Ma, <b>PeiwenSun</b>, Yaoting Wang, Di Hu
              <br>
              <br>
                <b>ECCV</b>, 2024
              <br>
              <a href="https://placeholder.pdf">Arxiv</a>
              <p></p>
              <p>A two-stage training strategy for AVS, which decomposes the AVSS task into two simple subtasks from localization to semantic understanding, which to achieve step-by-step global optimization.</p>
            </td>
          </tr>

       <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2022fewshot.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="...">
                    <papertitle> A Cascading Patch-based Learning Framework for Few-shot Classification  </papertitle>
                  </a>
                  <br>
      Wangding Zeng*, <b>PeiwenSun</b>* , Xinying Zhao, Guanlin Wu, Yang Tang, Honggang Zhang
                  <br>
                    *: equal contribution
                  <br>
                  <br>
                    <b>IJCNN 2024</b> &nbsp
                  <br>
                  <a href="...">Not yet released</a> 
                  <p></p>
                  <p>In this paper, we propose a series of strategies to train a better backbone for few-shot learn.</p>
                </td>
              </tr>

                 <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2023micad.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://link.springer.com/chapter/10.1007/978-981-97-1335-6_8">
                    <papertitle> Predicting Central Cervical Lymph Node Metastasis of Papillary Thyroid Carcinomas Using Multi-view Ultrasound Images  </papertitle>
                  </a>
                  <br>
                      Zishan Liu, <b>PeiwenSun</b>, Donghao Chen, Honggang Zhang, Yingying Li 
                  <br>
                  <br>
                    <b>MICAD 2023</b> &nbsp
                  <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-981-97-1335-6_8">Springer</a> 
                  <p></p>
                  <p>In this paper, the popular semantic segmentation network is firstly applied in thyroid nodule classification in ultrasound images.</p>
                </td>
              </tr>

	     <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2020MCM.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://github.com/PeiwenSun2000/2021-MCM-B-Finalist-sharing/blob/main/2100059-paper.pdf">
                    <papertitle>Race Against Fire  </papertitle>
                  </a>
                  <br>
			<b>PeiwenSun</b>, Wenjing Ye, Wenqing Yu
                  <br>
                  	<em>Mathematical Contest In Modeling</em>, 2021
                  <br>
                  <br>
                  	<b>Finalist Award</b> &nbsp 
                  <br>
                  <a href="https://github.com/PeiwenSun2000/2021-MCM-B-Finalist-sharing/blob/main/2100059-paper.pdf">github</a> 
                  <p></p>
                  <p>A modeling scheme for the layout and number of drones to extinguish and monitor wildfires in southern Australia.</p>
                </td>
              </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=_eYcd7WcjsVvbN1GXzPuSL6QzlZbVQeW0W-lxkWeoqM&cl=ffffff&w=a" width="300"></script>
</body>

</html>
