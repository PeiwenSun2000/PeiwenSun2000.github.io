<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Peiwen Sun</title>
  
  <meta name="author" content="Peiwen Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Peiwen Sun</name>
		<br>
              </p>
              <p>I am a Researcher at BUPT, focused on machine learning for emotion/sentiment understanding and multimodal understanding.  After I graduated from BUPT, I was recommended to the College of artificial intelligence in BUPT for a master's degree. Heading to my PhD in future.
		    </p>
		    <p>
              I used to be an intern at Megvii and Tencent to do researches and projects of emotion/sentiment understanding and multimodal understanding.
              </p>
              <p>
              
               
              </p>
              <p style="text-align:center">
                <a href="...">CV</a> &nbsp/&nbsp
    <a href="https://github.com/PeiwenSun2000"> GitHub </a> &nbsp/&nbsp 
    <a href="https://peiwensun2000.github.io/"> Page
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/PeiwenSun_headshot.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PeiwenSun_headshot.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<!---
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                2021
                  <ul>

                      <li>Intern at Tencent</li>
                      <li>Facial recognition and emtion/sentiment recognition</li>

                  </ul>
              </p>
              <p>
                2020
                  <ul>

                      <li>Intern at Magvii</li>
                      <li>Facial recognition and emtion/sentiment recognition</li>

                  </ul>
              </p>
            </td>
          </tr>
-->


          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Research</heading>
        
            <p>
      
              My research focuses on person recognition, emtion/sentiment recognition and multi-modal machine learning techniques for video recognition, including the use of sound and pictures to learn better representations.
            </p>
          </td>
            </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<!-- change file in here -->

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2023more.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://drive.google.com/file/d/1Qs85leGUAnEoBuxxgzUBrUX9soCn-DAl/view?usp=sharing">
                    <papertitle>More than Vanilla Fusion: a Simple, Decoupling-free, Attention Module for Multimodal fusion Based on Information Theory  </papertitle>
                  </a>
                  <br>
            <b>PeiwenSun</b>, Yifan Zhang, Zishan Liu, Donghao Chen, Honggang Zhang

                  <br>
                    <em>Submitted to ICASSP</em>, 2024
                  <br>
                  <a href="https://drive.google.com/file/d/1Qs85leGUAnEoBuxxgzUBrUX9soCn-DAl/view?usp=sharing">Google drive</a>
                  <p></p>
                  <p>This paper reconsiders the information fused in the multimodal case from a bionics perspective and proposes a simple, plug-and-play, attention module for vanilla fusion based on fundamental information theory and uncertainty theory.</p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2022fusion.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://drive.google.com/file/d/1_Dg1UUOCfRR0xHrVoOiahRqZkCYY7vQ8/view?usp=sharing">
                    <papertitle>A Method of Audio-Visual Person Verification by Mining Connections between Time Series  </papertitle>
                  </a>
                  <br>
            <b>PeiwenSun</b>, Shanshan Zhang, Zishan Liu, Yougen Yuan, Taotao Zhang, Honggang Zhang, Pengfei Hu

                  <br>
                    <em>interspeech</em>, 2023
                  <br>
                  <a href="https://drive.google.com/file/d/1_Dg1UUOCfRR0xHrVoOiahRqZkCYY7vQ8/view?usp=sharing">ISCA</a>
                  <p></p>
                  <p>A novel audio-visual strategy in person verification that considers connections between time series from a  generative perspective.</p>
                </td>
              </tr>

	     <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2020MCM.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://github.com/PeiwenSun2000/2021-MCM-B-Finalist-sharing/blob/main/2100059-paper.pdf">
                    <papertitle>Race Against Fire  </papertitle>
                  </a>
                  <br>
			<b>PeiwenSun</b>, Wenjing Ye, Wenqing Yu
                  <br>
                  	<em>Mathematical Contest In Modeling</em>, 2021
                  <br>
                  <br>
                  	<b>Finalist Award</b> &nbsp 
                  <br>
                  <a href="https://github.com/PeiwenSun2000/2021-MCM-B-Finalist-sharing/blob/main/2100059-paper.pdf">github</a> 
                  <p></p>
                  <p>A modeling scheme for the layout and number of drones to extinguish and monitor wildfires in southern Australia.</p>
                </td>
              </tr>

       <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/2022fewshot.png" alt="mvgpt" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="...">
                    <papertitle> A Cascading Patch-based Learning Framework for Few-shot Classification  </papertitle>
                  </a>
                  <br>
      Wangding Zeng*, <b>PeiwenSun</b>* , Xinying Zhao, Guanlin Wu, Yang Tang, Honggang Zhang
                  <br>
                    *: mutual contribution
                  <br>
                  <br>
                    <b>Still under revision</b> &nbsp
                  <br>
                  <a href="...">Not yet released</a> 
                  <p></p>
                  <p>In this paper, we propose a series of strategies to train a better backbone for few-shot learn.</p>
                </td>
              </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
